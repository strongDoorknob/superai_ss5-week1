{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Load tag mapping from tag_list.csv\n",
    "def load_tag_mapping(file_path):\n",
    "    tag2id = {}\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            tag, tag_id = row\n",
    "            tag2id[tag] = int(tag_id)\n",
    "    id2tag = {v: k for k, v in tag2id.items()}\n",
    "    return tag2id, id2tag\n",
    "\n",
    "\n",
    "# File paths - modify these to match your actual file locations\n",
    "tag_list_path = \"tag_list.csv\"\n",
    "train_folder = \"train/train\"\n",
    "eval_folder = \"eval/eval\"\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "os.makedirs(\"./logs\", exist_ok=True)\n",
    "os.makedirs(\"./fine_tuned_ner_model\", exist_ok=True)\n",
    "\n",
    "# Load tag mappings\n",
    "tag2id, id2tag = load_tag_mapping(tag_list_path)\n",
    "\n",
    "# Use tokenizer from the Thai NER model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"airesearch/wangchanberta-base-att-spm-uncased\", use_fast=True\n",
    ")\n",
    "\n",
    "\n",
    "# Function to load and prepare datasets\n",
    "def prepare_dataset(folder_path):\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                tokens, tags = [], []\n",
    "                for line in file:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        if tokens:\n",
    "                            all_tokens.append(tokens)\n",
    "                            all_tags.append(\n",
    "                                [tag if tag in tag2id else \"O\" for tag in tags]\n",
    "                            )\n",
    "                            tokens, tags = [], []\n",
    "                    else:\n",
    "                        parts = line.split(\"\\t\")\n",
    "                        if len(parts) == 4:\n",
    "                            word, _, ner_tag, _ = parts\n",
    "                            tokens.append(word)\n",
    "                            tags.append(ner_tag)\n",
    "                if tokens:\n",
    "                    all_tokens.append(tokens)\n",
    "                    all_tags.append(\n",
    "                        [tag if tag in tag2id else \"O\" for tag in tags]\n",
    "                    )\n",
    "\n",
    "    return all_tokens, all_tags\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "train_tokens, train_tags = prepare_dataset(train_folder)\n",
    "eval_tokens, eval_tags = prepare_dataset(eval_folder)\n",
    "\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_and_align_labels(tokens, tags):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        tokens,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(tokens)):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(\n",
    "                    tag2id.get(tags[i][word_idx], tag2id[\"O\"])\n",
    "                    if word_idx < len(tags[i])\n",
    "                    else -100\n",
    "                )\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "# Tokenize and align labels\n",
    "train_encodings = tokenize_and_align_labels(train_tokens, train_tags)\n",
    "eval_encodings = tokenize_and_align_labels(eval_tokens, eval_tags)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "\n",
    "train_dataset = NERDataset(train_encodings)\n",
    "eval_dataset = NERDataset(eval_encodings)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Load pre-trained model with updated number of labels\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"airesearch/wangchanberta-base-att-spm-uncased\", num_labels=len(tag2id)\n",
    ")\n",
    "\n",
    "\n",
    "# Metrics for evaluation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # Filter out -100 labels\n",
    "    true_labels = [\n",
    "        [id2tag[label] for label in sent if label != -100] for sent in labels\n",
    "    ]\n",
    "    true_preds = [\n",
    "        [id2tag[pred] for pred, label in zip(sent, labels[i]) if label != -100]\n",
    "        for i, sent in enumerate(preds)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_preds),\n",
    "        \"recall\": recall_score(true_labels, true_preds),\n",
    "        \"f1\": f1_score(true_labels, true_preds),\n",
    "    }\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Validation Results:\", eval_results)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./fine_tuned_ner_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_ner_model\")\n",
    "\n",
    "print(\"Model training and evaluation completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
